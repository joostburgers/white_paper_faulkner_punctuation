[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Faulkner and Punctuation",
    "section": "",
    "text": "The following is the ‘white paper’ for the essay ‘Faulkner and the Politics of the Long Sentence’ due to appear in The History of Punctuation in English. As the calculations for this project were quite involved, they have been separated into three different documents.\n\nText processing: This shows the standard pre-cleaning done before any textual analysis.\nOutlier Detection: This covers the calculations done to detect outliers in punctuation-mark usage in all of Faulkner’s fictions.\nSentence and Demographic Correlation: Shows the process of creating a demographic database and also running Pearson correlations between different variables.\n\nIn all cases, some raw data has been witheld. In particular this pertains to digitized copies of Faulkner’s texts, the posting of which would violate copyright. Secondarily, the demographic data extracted from the Digital Yoknapatawpha database has also been masked, even if the results are available. This data belongs to all the scholars who worked so hard to create it, and I do not have singular authority to distribute it to anyone without their consent. We are happy to release the data for scholarly purposes."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Faulkner and Punctuation",
    "section": "",
    "text": "The following is the ‘white paper’ for the essay ‘Faulkner and the Politics of the Long Sentence’ due to appear in The History of Punctuation in English. As the calculations for this project were quite involved, they have been separated into three different documents.\n\nText processing: This shows the standard pre-cleaning done before any textual analysis.\nOutlier Detection: This covers the calculations done to detect outliers in punctuation-mark usage in all of Faulkner’s fictions.\nSentence and Demographic Correlation: Shows the process of creating a demographic database and also running Pearson correlations between different variables.\n\nIn all cases, some raw data has been witheld. In particular this pertains to digitized copies of Faulkner’s texts, the posting of which would violate copyright. Secondarily, the demographic data extracted from the Digital Yoknapatawpha database has also been masked, even if the results are available. This data belongs to all the scholars who worked so hard to create it, and I do not have singular authority to distribute it to anyone without their consent. We are happy to release the data for scholarly purposes."
  },
  {
    "objectID": "text_processing_supplement_outliers.html",
    "href": "text_processing_supplement_outliers.html",
    "title": "Text Processing Supplement: Outliers",
    "section": "",
    "text": "Load in packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(qdapRegex )\n\nlibrary(EnvStats)\nlibrary(ggpubr)\n\nlibrary(univOutl)\n\n\n\n\nThe data preprocessing for this data set and that of the character matching in data set 2 is slightly different. Whereas that data set had to manually modified to align with the DY database, this data set is fuller and is a slightly less adulterated version of punctuation patterns in Faulkner. That said, it would be a mistake to suggest that this is the more “correct” version. It includes more texts.\n\n\nCode\nall_works_original &lt;-\n  list.files(file.path(\"data\"), full.names = TRUE, pattern = \"*.txt\") %&gt;% #grab a list of all the files with .txt extension\n  #the full.names value needs to set to TRUE to get the full path. For some reason you will get a \"permission denied\" error if you do not do this.\n  map_df(~ tibble(  #the map function performs the same command on all parts of the data set. In this case the .txt files\n    text = read_file(.), #read the files\n    date = ifelse(\n      str_detect(basename(.), \"[:digit:]{4}\") == TRUE,\n      str_extract(basename(.), \"[:digit:]{4}\"),\n      NA), #see if there is a date in yyyy format, i.e. 1987, and extract the date, if it can't find date NA\n    title=str_extract(basename(.), \"(?&lt;=_)[:alpha:]*\"),\n    code=str_extract(basename(.), \"[:upper:]+\"),\n    revised = str_detect(basename(.),\"_revised\")\n  ))"
  },
  {
    "objectID": "text_processing_supplement_outliers.html#precursors",
    "href": "text_processing_supplement_outliers.html#precursors",
    "title": "Text Processing Supplement: Outliers",
    "section": "",
    "text": "Load in packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(qdapRegex )\n\nlibrary(EnvStats)\nlibrary(ggpubr)\n\nlibrary(univOutl)\n\n\n\n\nThe data preprocessing for this data set and that of the character matching in data set 2 is slightly different. Whereas that data set had to manually modified to align with the DY database, this data set is fuller and is a slightly less adulterated version of punctuation patterns in Faulkner. That said, it would be a mistake to suggest that this is the more “correct” version. It includes more texts.\n\n\nCode\nall_works_original &lt;-\n  list.files(file.path(\"data\"), full.names = TRUE, pattern = \"*.txt\") %&gt;% #grab a list of all the files with .txt extension\n  #the full.names value needs to set to TRUE to get the full path. For some reason you will get a \"permission denied\" error if you do not do this.\n  map_df(~ tibble(  #the map function performs the same command on all parts of the data set. In this case the .txt files\n    text = read_file(.), #read the files\n    date = ifelse(\n      str_detect(basename(.), \"[:digit:]{4}\") == TRUE,\n      str_extract(basename(.), \"[:digit:]{4}\"),\n      NA), #see if there is a date in yyyy format, i.e. 1987, and extract the date, if it can't find date NA\n    title=str_extract(basename(.), \"(?&lt;=_)[:alpha:]*\"),\n    code=str_extract(basename(.), \"[:upper:]+\"),\n    revised = str_detect(basename(.),\"_revised\")\n  ))"
  },
  {
    "objectID": "text_processing_supplement_outliers.html#tidy-text",
    "href": "text_processing_supplement_outliers.html#tidy-text",
    "title": "Text Processing Supplement: Outliers",
    "section": "Tidy text",
    "text": "Tidy text\nThere are various features of the text files that cause issues with determining sentence length. They are abbreviations, titles with a period such as Mr. and Mrs., and ellipses. These have all been edited, so that the sentences in each text can be properly delimited.\n\n\nCode\nall_works_tidy_string_original &lt;- all_works_original %&gt;%\n  group_by(title, date, code) %&gt;%\n  mutate(text = rm_abbreviation(text, replacement = \"abbreviationremoved \")) %&gt;%\n  mutate(text = str_replace_all(text, \"Mr\\\\.\", \"Mr \")) %&gt;%\n  mutate(text = str_replace_all(text, \"Mrs\\\\.\", \"Mrs \")) %&gt;%\n  mutate(text = str_replace_all (text, \"\\\\.\\\\.\\\\.\", \" punctellipse \")) %&gt;%\n  mutate(text = str_replace_all (text, \"\\\\.\\\\s\\\\.\\\\s\\\\.\\\\s\", \" punctellipse \")) %&gt;%\n  mutate(text = str_replace_all(text, \"\\u2026\", \" punctellipse \")) %&gt;%\n  mutate(text = str_squish(text)) %&gt;%\n  mutate(text = str_replace_all(text, \"—\", \" - \")) %&gt;%\n  mutate(work_length = str_count(text, \"\\\\S+\")) %&gt;%\n  mutate(type = ifelse(work_length &gt; 40000, \"novel\", \"short_story\")) %&gt;%\n  mutate(cleaned = str_to_lower(text)) \n\n\n\n\nCode\nrevised_works_percent &lt;- all_works_tidy_string_original %&gt;% \n                        group_by(revised) %&gt;% \n                          summarise(work_length=sum(work_length)) %&gt;% \n                          mutate(percent= work_length/(sum(work_length)))"
  },
  {
    "objectID": "text_processing_supplement_outliers.html#descriptive-statistics",
    "href": "text_processing_supplement_outliers.html#descriptive-statistics",
    "title": "Text Processing Supplement: Outliers",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nThe relative make up of the corpus by text type.\n\n\nCode\n#Descriptive statistics. These are used in the introduction to the essay.\n\n#This calculates the percentage breakdown of the corpus. All functions have been left verbose for clarity.\n\ncorpus_percent_novel_short_story &lt;- all_works_tidy_string_original %&gt;%\n  group_by(type) %&gt;%\n  summarise (work_type_length = sum(work_length)) %&gt;%\n  ungroup() %&gt;%\n  mutate(percent = work_type_length / sum(work_type_length))\n\ncorpus_percent_novel_short_story %&gt;% \n  select(-work_type_length)\n\n\n# A tibble: 2 × 2\n  type        percent\n  &lt;chr&gt;         &lt;dbl&gt;\n1 novel         0.768\n2 short_story   0.232\n\n\n\n\nCode\ncorpus_percent_dy_nondy &lt;- all_works_tidy_string_original %&gt;%\n  mutate(type = ifelse(code == \"ZZ\", \"notDY\", \"DY\")) %&gt;%\n  group_by(type) %&gt;%\n  summarise (work_type_length = sum(work_length)) %&gt;%\n  ungroup() %&gt;%\n  mutate(percent = work_type_length / sum(work_type_length))\n\ncorpus_percent_dy_nondy\n\n\n# A tibble: 2 × 3\n  type  work_type_length percent\n  &lt;chr&gt;            &lt;int&gt;   &lt;dbl&gt;\n1 DY             1909505   0.709\n2 notDY           785318   0.291\n\n\n\n\nCode\ncorpus_percent_all &lt;- corpus_percent_novel_short_story %&gt;% \n                      bind_rows(corpus_percent_dy_nondy)\n\ncorpus_percent_all\n\n\n# A tibble: 4 × 3\n  type        work_type_length percent\n  &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;\n1 novel                2069683   0.768\n2 short_story           625140   0.232\n3 DY                   1909505   0.709\n4 notDY                 785318   0.291"
  },
  {
    "objectID": "text_processing_supplement_outliers.html#sentence-counts",
    "href": "text_processing_supplement_outliers.html#sentence-counts",
    "title": "Text Processing Supplement: Outliers",
    "section": "Sentence Counts",
    "text": "Sentence Counts\n\n\nCode\n#Create sentences using regex unnest. This works better than unnest_sentences in tidytext library, which drops all columns.\n\nall_works_punctuation &lt;-\n  all_works_tidy_string_original %&gt;%\n  ungroup() %&gt;%\n  group_by(title, date, code, type, revised) %&gt;%\n  unnest_regex(sentence, cleaned, \"[.?!]\") %&gt;%\n  mutate(sentence = str_replace(sentence, \"”(?=\\\\s{1})\", \"\")) %&gt;%\n  mutate (string_length = str_count(sentence, \"\\\\S+\")) %&gt;%\n  filter(string_length &gt; 0) %&gt;%\n  mutate(ellipse = str_count(sentence, \"punctellipse\")) %&gt;%\n  mutate(comma = str_count(sentence, \"\\\\,\")) %&gt;%\n  mutate(semi_colon = str_count(sentence, \"\\\\;\")) %&gt;%\n  mutate(dash = str_count(sentence, \"-\")) %&gt;%\n  mutate(colon = str_count(sentence, \"\\\\:\")) %&gt;%\n  mutate(parenthesis = str_count(sentence, \"[\\\\(\\\\)]\")) \n  \n\n# Even though it takes up unnecessary memory the sentence column is maintained to verify that the sentences are being parsed correctly.\n\n\n\n\nCode\n# These calculations were used to get a sense of how much duplicated text there is between GDM and the stories and likewise Unvanquished and the stories. This is not a whole lot.\n\nall_works_punctuation_duplicate &lt;- all_works_punctuation %&gt;% \n                                    ungroup()  %&gt;% \n                                      count(sentence) %&gt;% \n                                      filter(n&gt;1)\n\nall_works_unique_removed &lt;- all_works_punctuation %&gt;% \n                            right_join(all_works_punctuation_duplicate) %&gt;% \n                            group_by(title) %&gt;% \n                            distinct(sentence, .keep_all = TRUE)\n\n\nJoining with `by = join_by(sentence)`\n\n\nCode\nduplicate_sentences &lt;- all_works_unique_removed %&gt;% \n                        arrange(desc(string_length), sentence) %&gt;% \n                        filter(string_length&gt;6) %&gt;% \n                        ungroup() %&gt;% \n                        distinct(sentence,string_length) %&gt;% \n                        summarise(total_duplicates= sum(string_length)) \n\npercent_duplicate &lt;- duplicate_sentences %&gt;% \n                      mutate(duplicate_percent = total_duplicates/sum(revised_works_percent$work_length))"
  },
  {
    "objectID": "text_processing_supplement_outliers.html#summary-statistics-and-exploratory-data-analysis",
    "href": "text_processing_supplement_outliers.html#summary-statistics-and-exploratory-data-analysis",
    "title": "Text Processing Supplement: Outliers",
    "section": "Summary Statistics and Exploratory Data Analysis",
    "text": "Summary Statistics and Exploratory Data Analysis\nCreate punctuation table and remove Requiem because the punctuation patterns do not match that of a novel. This table is used continuously downstream. It produces the relative frequencies of punctuation in the corpus.\n\n\nCode\n# Create punctuation table and remove Requiem because the punctuation patterns do not match that of a novel. This table is used continuously downstream. It produces the relative frequencies of punctuation in the corpus.\n\nsummary_punctuation &lt;- all_works_punctuation %&gt;%\n  group_by(title, date, code, type) %&gt;%\n  filter(title != 'requiem') %&gt;% \n  summarise(across(string_length:parenthesis, ~ mean(.x))) \n\n\n`summarise()` has grouped output by 'title', 'date', 'code'. You can override\nusing the `.groups` argument.\n\n\nCode\nsummary_punctuation %&gt;% slice_max(10)\n\n\n# A tibble: 112 × 11\n# Groups:   title, date, code [112]\n   title  date  code  type  string_length ellipse comma semi_colon   dash  colon\n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Lo     1934  L     shor…          15.3 0.0646  1.02      0.108  0.0604 0.0854\n 2 absal… 1936  AA    novel          43.1 0.00192 2.38      0.285  0.504  0.238 \n 3 adang… 1930  ZZ    shor…          15.8 0.00513 1.14      0.108  0.179  0.0462\n 4 adast… 1931  AD    shor…          11.2 0.0153  0.812     0.0826 0.0505 0.0260\n 5 adole… 1922  ZZ    shor…          15.8 0.00826 1.08      0.129  0.129  0.0854\n 6 after… 1947  ZZ    shor…          29.4 0       1.75      0.169  0.305  0.0519\n 7 aljac… 1925  ZZ    shor…          19.2 0       0.955     0.0536 0.107  0.0804\n 8 allth… 1931  ADP   shor…          15.8 0.00688 1.03      0.0436 0.0665 0.0459\n 9 ambus… 1934  AMB   shor…          12.1 0.00855 0.765     0.0919 0.109  0.0321\n10 aretu… 1938  ZZ    shor…          23.3 0.0113  1.28      0.0994 0.235  0.0750\n# ℹ 102 more rows\n# ℹ 1 more variable: parenthesis &lt;dbl&gt;\n\n\nThis calculates the percent long sentences for each work. A long sentence is any sentence that exceeds the corpus average.\n\n\nCode\n#This calculates the percent long sentences for each work. A long sentence is any sentence that exceeds the corpus average (corpus_mean_string_length)\n\nsentence_length_by_work &lt;- all_works_punctuation %&gt;%\n  group_by(title, code) %&gt;%\n  summarise(average_string_length = mean(string_length))\n\n\n`summarise()` has grouped output by 'title'. You can override using the\n`.groups` argument.\n\n\nCode\ncorpus_mean_string_length &lt;- sentence_length_by_work %&gt;%\n  ungroup() %&gt;%\n  summarise(corpus_mean_string_length = mean(average_string_length)) %&gt;%\n  pull(corpus_mean_string_length)\n\nsentence_length_percentage &lt;- mean(sentence_length_by_work$average_string_length &gt; corpus_mean_string_length)\n\nsentence_long_short_percent &lt;- all_works_punctuation %&gt;%\n  group_by(title, code) %&gt;%\n  summarise(\n    all_sentence = n(),\n    long_sentences = sum(string_length &gt; corpus_mean_string_length)\n  ) %&gt;%\n  mutate(percent_long = long_sentences / all_sentence)\n\n\n`summarise()` has grouped output by 'title'. You can override using the\n`.groups` argument.\n\n\nCode\nsentence_long_short_percent\n\n\n# A tibble: 113 × 5\n# Groups:   title [113]\n   title            code  all_sentence long_sentences percent_long\n   &lt;chr&gt;            &lt;chr&gt;        &lt;int&gt;          &lt;int&gt;        &lt;dbl&gt;\n 1 Lo               L              480            140        0.292\n 2 absalomcorrect   AA            3130           1595        0.510\n 3 adangerousman    ZZ             195             65        0.333\n 4 adastra          AD             654            129        0.197\n 5 adolesence       ZZ             363            149        0.410\n 6 afternooncow     ZZ             154             83        0.539\n 7 aljackson        ZZ             112             49        0.438\n 8 allthedeadpilots ADP            436            143        0.328\n 9 ambuscade        AMB            468            112        0.239\n10 areturn          ZZ             533            175        0.328\n# ℹ 103 more rows"
  },
  {
    "objectID": "text_processing_supplement_outliers.html#outlier-analysis",
    "href": "text_processing_supplement_outliers.html#outlier-analysis",
    "title": "Text Processing Supplement: Outliers",
    "section": "Outlier analysis",
    "text": "Outlier analysis\nThe following procedures were all performed on the various punctuation-marks. The results were, in turn, consolidated into one table, which was used in the paper.\n\nString Length\n\nDistribution Chart\n\n\nCode\nsummary_punctuation %&gt;% \nggplot(aes(x=string_length)) + \n  geom_histogram(aes(y=after_stat(density)), colour=\"black\", fill=\"white\")+\n  geom_density(alpha=.2, fill=\"#FF6666\")+\n  ggtitle(\"String Length Distribution\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQQ Plot\n\n\nCode\nstring_length_shapiro &lt;- shapiro.test(log1p(summary_punctuation$string_length))\n\nggqqplot(log1p(summary_punctuation$string_length), xlab = \"Text\", title = paste(\"String Length Distribution\", \"Shapiro P value &lt; \", round(string_length_shapiro$p.value,6))\n         )\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\nstring_length_outlier &lt;- boxplot.stats(log1p(summary_punctuation$string_length))$out\nstring_length_outlier_rownumbers &lt;- which(log1p(summary_punctuation$string_length) %in% c(string_length_outlier))\n\nstring_length_test &lt;- rosnerTest(log1p(summary_punctuation$string_length),\n  k = length(string_length_outlier_rownumbers)\n)\nstring_length_outlier_obs &lt;-  string_length_test$all.stats %&gt;% \n                       filter(Outlier ==TRUE) %&gt;% \n                       select(Obs.Num)\n\n#Write value to punctuation results\n\nstring_length_outlier_result &lt;- summary_punctuation[string_length_outlier_obs$Obs.Num,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, string_length)\n\n\n\n\nAdjusted Boxplot\n\n\nCode\nstring_length_adjusted_boxplot &lt;-\n  boxB(summary_punctuation$string_length, method = \"adjbox\")\n\n\nWarning in boxB(summary_punctuation$string_length, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe default of 'doScale' is FALSE now for stability;\n  set options(mc_doScale_quiet=TRUE) to suppress this (once per session) message\n\n\nThe MedCouple skewness measure is: 0.107\n\n\nNo. of outliers in left tail: 0\n\n\nNo. of outliers in right tail: 3\n\n\nCode\nstring_length_boxplot_outlier_result &lt;-\n  summary_punctuation[string_length_adjusted_boxplot$outliers, ] %&gt;%\n  ungroup() %&gt;%\n  select(title, string_length)\n\n\n\n\n\nEllipse\n\nDistribution\n\n\nCode\nsummary_punctuation %&gt;%\n  ggplot(aes(x = ellipse)) +\n  geom_histogram(aes(y = ..density..),\n                 colour = \"black\",\n                 fill = \"white\") +\n  geom_density(alpha = .2, fill = \"#FF6666\") +\n  ggtitle(\"Ellipse Distribution\")\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQQ Plot\n\n\nCode\nellipse_shapiro &lt;- shapiro.test(1/summary_punctuation$ellipse)\n\nggqqplot(\n  log1p(summary_punctuation$ellipse),\n  xlab = \"Text\",\n  title = paste(\n    \"Ellipse Distribution\",\n    \"Shapiro P value &lt; \",\n    round(ellipse_shapiro$p.value, 6)\n  )\n)\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\n#I tried working with the inverse here, but got mixed results. It pulls out all the low observations as unusual instead of simply seeing them as zero. It looks like both with the lognormal and the regular the top still clusters as outliers. This makes logically the most sense even if the data is pretty noisy and it is unclear how much value to attach to this.\n\n\n\nellipse_outlier &lt;-\n  boxplot.stats(log1p(summary_punctuation$ellipse))$out\nellipse_outlier_rownumbers &lt;-\n  which((log1p(summary_punctuation$ellipse)) %in% c(ellipse_outlier))\n\nellipse_test &lt;- rosnerTest((log1p(summary_punctuation$ellipse)),\n                           k = length(ellipse_outlier_rownumbers))\nellipse_outlier_obs &lt;-  ellipse_test$all.stats %&gt;%\n  filter(Outlier == TRUE) %&gt;%\n  select(Obs.Num)\n\nellipse_outlier_result &lt;-\n  summary_punctuation[ellipse_outlier_obs$Obs.Num, ] %&gt;%\n  ungroup() %&gt;%\n  select(title, ellipse)\n\n\n\n\nAdjusted Boxplot\n\n\nCode\nellipse_adjusted_boxplot &lt;-\n  boxB(summary_punctuation$ellipse, method = \"adjbox\")\n\n\nWarning in boxB(summary_punctuation$ellipse, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe MedCouple skewness measure is: 0.3513\n\n\nNo outliers found\n\n\nCode\nellipse_boxplot_outlier_result &lt;-\n  summary_punctuation[ellipse_adjusted_boxplot$outliers, ] %&gt;%\n  ungroup() %&gt;%\n  select(title, ellipse)\n\n\n\n\nCode\n#Simple test to verify the most numerous result. As suspected, the top 5 are somewhat unexpected.\n\nsummary_punctuation %&gt;% \n  ungroup() %&gt;% \n  # slice_max(ellipse,n=10) %&gt;% \n \n  ggplot( aes(x=date, y=ellipse)) + \n  geom_bar(stat = \"identity\",position = position_dodge2(preserve = \"single\"))\n\n\n\n\n\n\n\nCode\n# correlation_ellipse_short_story &lt;- summary_punctuation %&gt;%\n#   filter(ellipse&gt;0) %&gt;% \n#   with(cor.test(date, ellipse))\n# \n# correlation_ellipse_short_story\n\n\n\n\n\nComma\n\nDistribution\n\n\nCode\nsummary_punctuation %&gt;%\n  ggplot(aes(x = comma)) +\n  geom_histogram(aes(y = ..density..),\n                 colour = \"black\",\n                 fill = \"white\") +\n  geom_density(alpha = .2, fill = \"#FF6666\") +\n  ggtitle(\"comma Distribution\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQQ Plot\n\n\nCode\ncomma_shapiro &lt;- shapiro.test(summary_punctuation$ellipse)\n\nggqqplot(summary_punctuation$comma, xlab = \"Text\", title = paste(\"Comma Distribution\", \"Shapiro P value &lt; \", round(comma_shapiro$p.value,6))\n         )\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\ncomma_outlier &lt;- boxplot.stats(summary_punctuation$comma)$out\ncomma_outlier_rownumbers &lt;-\n  which(summary_punctuation$comma %in% c(comma_outlier))\n\ncomma_test &lt;- rosnerTest(summary_punctuation$comma,\n                         k = length(comma_outlier_rownumbers))\n\ncomma_outlier_obs &lt;-  comma_test$all.stats %&gt;%\n  filter(Outlier == TRUE) %&gt;%\n  select(Obs.Num)\n\ncomma_outlier_result &lt;- summary_punctuation[comma_outlier_obs$Obs.Num,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, comma)\n\n\n\n\nAdjusted Boxplot\n\n\nCode\ncomma_adjusted_boxplot &lt;-\n  boxB(summary_punctuation$comma, method = \"adjbox\")\n\n\nWarning in boxB(summary_punctuation$comma, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe MedCouple skewness measure is: 0.1514\n\n\nNo. of outliers in left tail: 2\n\n\nNo. of outliers in right tail: 2\n\n\nCode\ncomma_boxplot_outlier_result &lt;-\n  summary_punctuation[comma_adjusted_boxplot$outliers, ] %&gt;%\n  ungroup() %&gt;%\n  select(title, comma)\n\n\n\n\n\nSemi-Colon\n\nDistribution\n\n\nCode\nsummary_punctuation %&gt;%\n  ggplot(aes(x = semi_colon)) +\n  geom_histogram(aes(y = ..density..),\n                 colour = \"black\",\n                 fill = \"white\") +\n  geom_density(alpha = .2, fill = \"#FF6666\") +\n  ggtitle(\"Semi-Colon Distribution\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQQ Plot\n\n\nCode\nsemi_colon_shapiro &lt;- shapiro.test(summary_punctuation$semi_colon)\n\nggqqplot(\n  log1p(summary_punctuation$semi_colon),\n  xlab = \"Text\",\n  title = paste(\n    \"Semi-Colon's Per Word\",\n    \"Shapiro P value &lt; \",\n    round(semi_colon_shapiro$p.value, 6)\n  )\n)\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\nsemi_colon_outlier &lt;-\n  boxplot.stats(log1p(summary_punctuation$semi_colon))$out\nsemi_colon_outlier_rownumbers &lt;-\n  which(log1p(summary_punctuation$semi_colon) %in% c(semi_colon_outlier))\n\n\nsemi_colon_test &lt;- rosnerTest(log1p(summary_punctuation$semi_colon),\n                              k = length(semi_colon_outlier_rownumbers))\nsemi_colon_outlier_obs &lt;-  semi_colon_test$all.stats %&gt;%\n  filter(Outlier == TRUE) %&gt;%\n  select(Obs.Num)\n\nsemi_colon_outlier_result &lt;-\n  summary_punctuation[semi_colon_outlier_obs$Obs.Num, ] %&gt;%\n  ungroup() %&gt;%\n  select(title, semi_colon)\n\n\n\n\nAdjusted Box Plot\n\n\nCode\nsemi_colon_adjusted_boxplot &lt;- boxB(summary_punctuation$semi_colon, method=\"adjbox\")\n\n\nWarning in boxB(summary_punctuation$semi_colon, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe MedCouple skewness measure is: 0.2435\n\n\nNo. of outliers in left tail: 1\n\n\nNo. of outliers in right tail: 0\n\n\nCode\nsemi_colon_boxplot_outlier_result &lt;- summary_punctuation[semi_colon_adjusted_boxplot$outliers,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, semi_colon)\n\n\n\n\n\nDash\n\nDistribution\n\n\nCode\nsummary_punctuation %&gt;% \n\nggplot(aes(x=dash)) +\n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n  geom_density(alpha=.2, fill=\"#FF6666\")+\n   ggtitle(\"Dash Distribution\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nDash QQ Plot\n\n\nCode\ndash_shapiro &lt;- shapiro.test(log1p(summary_punctuation$dash))\n\nggqqplot(log1p(summary_punctuation$dash), xlab = \"Text\", title = paste(\"Dashes Per Word\", \"Shapiro P value &lt; \", round(dash_shapiro$p.value,6))\n         )\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\ndash_outlier &lt;- boxplot.stats(log1p(summary_punctuation$dash))$out\ndash_outlier_rownumbers &lt;- which(log1p(summary_punctuation$dash) %in% c(dash_outlier))\n\ndash_test &lt;- rosnerTest(log1p(summary_punctuation$dash),\n  k = length(dash_outlier_rownumbers)\n)\ndash_outlier_obs &lt;-  dash_test$all.stats %&gt;% \n                       filter(Outlier ==TRUE) %&gt;% \n                       select(Obs.Num)\n\ndash_outlier_result &lt;- summary_punctuation[dash_outlier_obs$Obs.Num,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, dash)\n\n\n\n\nAdjusted Boxplot\n\n\nCode\ndash_adjusted_boxplot &lt;- boxB(summary_punctuation$dash, method=\"adjbox\")\n\n\nWarning in boxB(summary_punctuation$dash, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe MedCouple skewness measure is: 0.3637\n\n\nNo. of outliers in left tail: 5\n\n\nNo. of outliers in right tail: 0\n\n\nCode\ndash_boxplot_outlier_result &lt;- summary_punctuation[dash_adjusted_boxplot$outliers,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, dash)\n\n\n\n\n\nColon\n\nDistribution\n\n\nCode\nsummary_punctuation %&gt;% \nggplot(aes(x = colon)) + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n  geom_density(alpha=.2, fill=\"#FF6666\")+\n  ggtitle(\"Colon Distribution\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQQ Plot\n\n\nCode\ncolon_shapiro &lt;- shapiro.test(1/summary_punctuation$colon)\n\nggqqplot(log1p(summary_punctuation$colon), xlab = \"Text\", title = paste(\"Colons Per Word\", \"Shapiro P value &lt; \", round(colon_shapiro$p.value,6))\n         )\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\ncolon_outlier &lt;- boxplot.stats(log1p(summary_punctuation$colon))$out\ncolon_outlier_rownumbers &lt;- which(log1p(summary_punctuation$colon) %in% c(colon_outlier))\n\ncolon_test &lt;- rosnerTest(log1p(summary_punctuation$colon),\n  k = length(colon_outlier_rownumbers)\n)\ncolon_outlier_obs &lt;-  colon_test$all.stats %&gt;% \n                       filter(Outlier ==TRUE) %&gt;% \n                       select(Obs.Num)\n\ncolon_outlier_result &lt;- summary_punctuation[colon_outlier_obs$Obs.Num,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, colon)\n\n\n\n\nColon Adjusted Box Plot\n\n\nCode\ncolon_adjusted_boxplot &lt;- boxB(summary_punctuation$colon, method=\"adjbox\")\n\n\nWarning in boxB(summary_punctuation$colon, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe MedCouple skewness measure is: 0.2636\n\n\nNo. of outliers in left tail: 0\n\n\nNo. of outliers in right tail: 3\n\n\nCode\ncolon_boxplot_outlier_result &lt;- summary_punctuation[colon_adjusted_boxplot$outliers,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, colon)\n\n\n\n\n\nParenthesis\n\nDistribution\n\n\nCode\nsummary_punctuation %&gt;% \nggplot(aes(x = parenthesis)) + \n geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\")+\n  geom_density(alpha=.2, fill=\"#FF6666\")+\n  ggtitle(\"Parenthesis Distribution\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nQQ Plot\n\n\nCode\nparenthesis_shapiro &lt;- shapiro.test(log1p(summary_punctuation$parenthesis))\n\nggqqplot(log1p(summary_punctuation$parenthesis), xlab = \"Text\", title = paste(\"parenthesiss Per Word\", \"Shapiro P value &lt; \", round(parenthesis_shapiro$p.value,6))\n         )\n\n\n\n\n\n\n\nRosner Test\n\n\nCode\nparenthesis_outlier &lt;- boxplot.stats(log1p(summary_punctuation$parenthesis))$out\nparenthesis_outlier_rownumbers &lt;- which(log1p(summary_punctuation$parenthesis) %in% c(parenthesis_outlier))\n\nparenthesis_test &lt;- rosnerTest(log1p(summary_punctuation$parenthesis),\n  k = length(parenthesis_outlier_rownumbers)\n)\nparenthesis_outlier_obs &lt;-  parenthesis_test$all.stats %&gt;% \n                       filter(Outlier ==TRUE) %&gt;% \n                       select(Obs.Num)\n\nparenthesis_outlier_result &lt;- summary_punctuation[parenthesis_outlier_obs$Obs.Num,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, parenthesis)\n\n\n\n\nAdjusted Box Plot\n\n\nCode\nparenthesis_adjusted_boxplot &lt;- boxB(summary_punctuation$parenthesis, method=\"adjbox\")\n\n\nWarning in boxB(summary_punctuation$parenthesis, method = \"adjbox\"): With\nmethod='adjbox' the argument k is set equal to 1.5\n\n\nThe MedCouple skewness measure is: 0.4385\n\n\nNo. of outliers in left tail: 0\n\n\nNo. of outliers in right tail: 4\n\n\nCode\nparenthesis_boxplot_outlier_result &lt;- summary_punctuation[parenthesis_adjusted_boxplot$outliers,] %&gt;% \n                                ungroup() %&gt;% \n                                select(title, parenthesis)\n\n\n\n\nCode\npunctuation_results &lt;- summary_punctuation %&gt;% \n                        select(title:type)\n\n\n\n\n\nAll Outliers\n\n\nCode\n# Create punctuation result\npunctuation_results_full_table &lt;- NULL\npunctuation_results_full_table &lt;-  punctuation_results %&gt;% \n  left_join(string_length_outlier_result, by = join_by(title)) %&gt;% \n  rename(string_length_rosner = string_length) %&gt;% \n  left_join(string_length_boxplot_outlier_result, by = join_by(title)) %&gt;%\n  rename(string_length_boxplot = string_length) %&gt;% \n  left_join(ellipse_outlier_result, by = join_by(title)) %&gt;%\n  rename(ellipse_rosner = ellipse) %&gt;% \n  left_join(ellipse_boxplot_outlier_result, by = join_by(title)) %&gt;% \n  rename(ellipse_boxplot = ellipse) %&gt;% \n  left_join(comma_outlier_result, by = join_by(title)) %&gt;%\n  rename(comma_rosner = comma) %&gt;% \n  left_join(comma_boxplot_outlier_result, by = join_by(title)) %&gt;%\n  rename(comma_boxplot = comma) %&gt;% \n  left_join(semi_colon_outlier_result, by = join_by(title)) %&gt;%\n  rename(semi_colon_rosner = semi_colon) %&gt;% \n  left_join(semi_colon_boxplot_outlier_result, by = join_by(title)) %&gt;% \n  rename(semi_colon_boxplot = semi_colon) %&gt;% \n  left_join(dash_outlier_result, by = join_by(title)) %&gt;% \n  rename(dash_rosner = dash) %&gt;% \n  left_join(dash_boxplot_outlier_result, by = join_by(title)) %&gt;% \n  rename(dash_boxplot = dash) %&gt;% \n  left_join(colon_outlier_result, by = join_by(title)) %&gt;% \n  rename(colon_rosner = colon) %&gt;% \n  left_join(colon_boxplot_outlier_result, by = join_by(title)) %&gt;% \n  rename(colon_boxplot = colon) %&gt;% \n  left_join(parenthesis_outlier_result, by = join_by(title)) %&gt;% \n  rename(parenthesis_rosner = parenthesis) %&gt;% \n  left_join(parenthesis_boxplot_outlier_result, by = join_by(title)) %&gt;% \n  rename(parenthesis_boxplot = parenthesis)\n\n\n\n\nCode\nall_outliers &lt;- punctuation_results_full_table %&gt;% \n  rowwise() %&gt;% \n  mutate(outlier = sum(c_across(where(is.numeric)), na.rm = T)) %&gt;% \n  mutate(outlier = ifelse(outlier&gt;0,TRUE,FALSE)) %&gt;% \n  filter(outlier == TRUE)  %&gt;% \n  mutate_at(c(8,11), as.numeric)\n\n\n\n\nCode\nall_outliers_count &lt;- all_outliers %&gt;% \n    select(5:18) %&gt;%  \n    is.na %&gt;% \n    `!` %&gt;% \n    rowSums\n\n\nAdding missing grouping variables: `title`, `date`, `code`\n\n\nCode\nall_outliers_count &lt;- all_outliers %&gt;% \n  add_column(all_outliers_count)\n\n\nall_outliers_count\n\n\n# A tibble: 16 × 20\n# Rowwise:  title, date, code\n   title            date  code  type  string_length_rosner string_length_boxplot\n   &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Lo               1934  L     shor…                 NA                    NA  \n 2 absalomcorrect   1936  AA    novel                 NA                    43.1\n 3 afternooncow     1947  ZZ    shor…                 NA                    NA  \n 4 beyond           1933  BE    shor…                 NA                    NA  \n 5 elly             1934  ELY   shor…                 NA                    NA  \n 6 fable            1954  ZZ    novel                 NA                    NA  \n 7 hogpawn          1955  ZZ    shor…                 NA                    NA  \n 8 intruder         1949  ID    novel                 NA                    38.8\n 9 misszilphiagant  1932  MZG   shor…                 NA                    NA  \n10 mistral          1931  ZZ    shor…                 NA                    NA  \n11 requiemcorrected 1951  RQ    novel                 NA                    NA  \n12 sepulturesouth   1954  ZZ    shor…                 54.3                  54.3\n13 soundcorrected   1929  SF    novel                 NA                    NA  \n14 thateveningsun   1931  TES   shor…                 NA                    NA  \n15 thepriest        1925  ZZ    shor…                 NA                    NA  \n16 twodollarwife    1936  ZZ    shor…                 NA                    NA  \n# ℹ 14 more variables: ellipse_rosner &lt;dbl&gt;, ellipse_boxplot &lt;dbl&gt;,\n#   comma_rosner &lt;dbl&gt;, comma_boxplot &lt;dbl&gt;, semi_colon_rosner &lt;dbl&gt;,\n#   semi_colon_boxplot &lt;dbl&gt;, dash_rosner &lt;dbl&gt;, dash_boxplot &lt;dbl&gt;,\n#   colon_rosner &lt;dbl&gt;, colon_boxplot &lt;dbl&gt;, parenthesis_rosner &lt;dbl&gt;,\n#   parenthesis_boxplot &lt;dbl&gt;, outlier &lt;lgl&gt;, all_outliers_count &lt;dbl&gt;"
  }
]